{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a2c534-c854-4afa-8d8b-f2d0996485be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data source for sandbox\n",
      "Loading data from 2022-05-28 to 2025-06-02 (oneâ€time)...\n",
      "â†’ Running PCA for 2025-06-02...\n",
      "ğŸƒ View run PCA_2025-06-02 at: http://127.0.0.1:8768/#/experiments/1462/runs/dc7ed805ae784e2483000989b6c83340\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1462\n",
      "ğŸƒ View run Rolling PCA at: http://127.0.0.1:8768/#/experiments/1462/runs/e487699c842443869fb7d66c27533875\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1462\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'raw_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 232\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m     as_of \u001b[38;5;241m=\u001b[39m default_as_of\n\u001b[0;32m--> 232\u001b[0m \u001b[43mpopulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault_backdated_days\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_of\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 177\u001b[0m, in \u001b[0;36mpopulate\u001b[0;34m(days, as_of)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m mlflow_start_time_per_slice\n\u001b[1;32m    176\u001b[0m mlflow_start_time_per_slice \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime(),)  \u001b[38;5;66;03m# just to measure perâ€slice latency\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m explained_ratio \u001b[38;5;241m=\u001b[39m \u001b[43mrun_pca_and_log_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mas_of_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpivot_filled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m scree_data\u001b[38;5;241m.\u001b[39mappend((as_of_date, explained_ratio))\n\u001b[1;32m    179\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mend_run()\n",
      "Cell \u001b[0;32mIn[3], line 86\u001b[0m, in \u001b[0;36mrun_pca_and_log_slice\u001b[0;34m(as_of_date, pivot_filled)\u001b[0m\n\u001b[1;32m     77\u001b[0m total_var \u001b[38;5;241m=\u001b[39m ((X \u001b[38;5;241m-\u001b[39m means) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Fit PCA. If using sklearn, you'd do something like:\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# sklearn_pca = SklearnPCA(n_components=N_COMPONENTS)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# all_scores = sklearn_pca.fit_transform(X)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# If you stick with legacy_pca, assume it returns (components, explained_ratio, mean_curve, all_scores).\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m components, explained_ratio, mean_curve, all_scores \u001b[38;5;241m=\u001b[39m \u001b[43mpca_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_COMPONENTS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m today_scores \u001b[38;5;241m=\u001b[39m all_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# last row corresponds to as_of_date\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Compute reconstruction error & RÂ²\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/code/models/pca_model.py:72\u001b[0m, in \u001b[0;36msklearn_pca\u001b[0;34m(X_np, n_components)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Initialize an sklearn PCA object. 'svd_solver=\"auto\"' will pick the best method;\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# for large matrices you could swap to 'randomized' explicitly, but 'auto' usually does the right thing.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39mn_components, svd_solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, whiten\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m mlflow\u001b[38;5;241m.\u001b[39msklearn\u001b[38;5;241m.\u001b[39mlog_model(\n\u001b[0;32m---> 72\u001b[0m     sk_model\u001b[38;5;241m=\u001b[39m\u001b[43mraw_model\u001b[49m,\n\u001b[1;32m     73\u001b[0m     artifact_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemo_pca_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     74\u001b[0m     registered_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDemoPcaModel\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Fit + transform in one shot (centers X_np internally, uses C/Fortran routines for SVD)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m scores \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(X_np)           \u001b[38;5;66;03m# shape = (n_samples, n_components)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_model' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "\n",
    "import data.data_source as data_source\n",
    "\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "from datetime import date\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "from config import env\n",
    "\n",
    "from models.pca_model import legacy_pca, sklearn_pca\n",
    "\n",
    "# â”€â”€â”€ CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TENORS = [0.25, 0.5, 1, 2, 3, 5, 7, 10, 20, 30]\n",
    "ROLLING_YEARS = 3\n",
    "N_COMPONENTS = 3\n",
    "CURVE_TYPE = \"US Treasury Par\"\n",
    "pca_model = sklearn_pca\n",
    "\n",
    "# MLflow experiment\n",
    "experiment_name = f\"PCA Training [{env}]\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# â”€â”€â”€ DATASOURCE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ds = data_source.get_data_source()\n",
    "\n",
    "# â”€â”€â”€ ONEâ€TIME LOAD & PIVOT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_and_pivot_all(earliest_date: date, latest_date: date) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pull every curve row between earliest_date and latest_date once,\n",
    "    pivot to a DateÃ—Tenor matrix, then forward/backfill missing values\n",
    "    across the entire range. Return a pivoted DataFrame with tenor columns.\n",
    "    \"\"\"\n",
    "    sql = f\"\"\"\n",
    "        SELECT curve_date, tenor_num AS tenor, rate\n",
    "          FROM rate_curves\n",
    "         WHERE curve_date BETWEEN '{earliest_date}'::date AND '{latest_date}'::date\n",
    "           AND curve_type = '{CURVE_TYPE}'\n",
    "        ORDER BY curve_date\n",
    "    \"\"\"\n",
    "    df_all = ds.query(sql).to_pandas()\n",
    "    df_all[\"curve_date\"] = pd.to_datetime(df_all[\"curve_date\"])\n",
    "    # pivot once\n",
    "    pivot = df_all.pivot(index=\"curve_date\", columns=\"tenor\", values=\"rate\")\n",
    "    # ensure all TENORS are present\n",
    "    pivot = pivot.reindex(columns=TENORS)\n",
    "    # forwardâ€fill & backâ€fill entire matrix\n",
    "    pivot_filled = pivot.ffill().bfill()\n",
    "    return pivot_filled\n",
    "\n",
    "# â”€â”€â”€ PCAâ€ANDâ€LOG FOR A SLICE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_pca_and_log_slice(as_of_date: date, pivot_filled: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Perform PCA on the slice of pivot_filled from (as_of_date - 3y) to as_of_date.\n",
    "    Insert the results into the DB and log metrics/artifacts to MLflow.\n",
    "    \"\"\"\n",
    "    start_date = as_of_date - relativedelta(years=ROLLING_YEARS)\n",
    "    end_date = as_of_date\n",
    "\n",
    "    # Extract the subâ€matrix for this date range:\n",
    "    # because we've already forward/backfilled, this slice has no NaNs.\n",
    "    slice_df = pivot_filled.loc[start_date:end_date]\n",
    "    X = slice_df.to_numpy()\n",
    "    num_obs, num_tenors = X.shape\n",
    "    means = X.mean(axis=0)\n",
    "    total_var = ((X - means) ** 2).mean()\n",
    "\n",
    "    # Fit PCA. If using sklearn, you'd do something like:\n",
    "    # sklearn_pca = SklearnPCA(n_components=N_COMPONENTS)\n",
    "    # all_scores = sklearn_pca.fit_transform(X)\n",
    "    # components = sklearn_pca.components_\n",
    "    # explained_ratio = sklearn_pca.explained_variance_ratio_\n",
    "    #\n",
    "    # If you stick with legacy_pca, assume it returns (components, explained_ratio, mean_curve, all_scores).\n",
    "    components, explained_ratio, mean_curve, all_scores = pca_model(X, N_COMPONENTS)\n",
    "    today_scores = all_scores[-1]  # last row corresponds to as_of_date\n",
    "\n",
    "    # Compute reconstruction error & RÂ²\n",
    "    X_recon = all_scores @ components + mean_curve\n",
    "    mse = ((X - X_recon) ** 2).mean()\n",
    "    r2 = 1 - mse / total_var\n",
    "\n",
    "    total_explained = float(explained_ratio.sum())\n",
    "\n",
    "    # INSERT/UPSERT into DB\n",
    "    run_id = str(uuid.uuid4())\n",
    "    insert_sql = f\"\"\"\n",
    "    INSERT INTO pca_results (\n",
    "      run_id, curve_type, curve_date, n_components,\n",
    "      total_explained_variance_ratio, explained_variance_ratios,\n",
    "      mean_curve, components, scores\n",
    "    ) VALUES (\n",
    "      '{run_id}', '{CURVE_TYPE}', '{as_of_date}',\n",
    "      {N_COMPONENTS}, {total_explained},\n",
    "      ARRAY{explained_ratio.tolist()},\n",
    "      ARRAY{mean_curve.tolist()},\n",
    "      '{json.dumps(components.tolist()).replace(\"'\", \"''\")}',\n",
    "      ARRAY{today_scores.tolist()}\n",
    "    )\n",
    "    ON CONFLICT (curve_type, curve_date)\n",
    "    DO UPDATE SET\n",
    "      run_id                        = EXCLUDED.run_id,\n",
    "      run_timestamp                 = CLOCK_TIMESTAMP(),\n",
    "      n_components                  = EXCLUDED.n_components,\n",
    "      total_explained_variance_ratio= EXCLUDED.total_explained_variance_ratio,\n",
    "      explained_variance_ratios     = EXCLUDED.explained_variance_ratios,\n",
    "      mean_curve                    = EXCLUDED.mean_curve,\n",
    "      components                    = EXCLUDED.components,\n",
    "      scores                        = EXCLUDED.scores;\n",
    "    \"\"\"\n",
    "    ds.query(insert_sql)\n",
    "\n",
    "    # MLflow logging for this slice\n",
    "    mlflow.log_param(\"as_of_date\", as_of_date)\n",
    "    mlflow.log_param(\"num_tenors\", num_tenors)\n",
    "    mlflow.log_param(\"num_observations\", num_obs)\n",
    "\n",
    "    mlflow.log_metric(\"reconstruction_mse\", float(mse))\n",
    "    mlflow.log_metric(\"total_explained_variance\", total_explained)\n",
    "    for i, ratio in enumerate(explained_ratio, start=1):\n",
    "        mlflow.log_metric(f\"explained_variance_ratio_{i}\", float(ratio))\n",
    "    mlflow.log_metric(\"run_duration_seconds\", time.time() - mlflow_start_time_per_slice[0])\n",
    "\n",
    "    return explained_ratio  # return this so we can build the scree plot later\n",
    "\n",
    "# â”€â”€â”€ POPULATE LOOP (ONEâ€TIME LOAD + SLICE) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def populate(days: int, as_of: date):\n",
    "    \"\"\"\n",
    "    Instead of calling load_curve_data 1Ã—/day, we:\n",
    "    1) Compute the earliest date weâ€™ll need (3 years + days back).  \n",
    "    2) Pull everything once, pivot & fill.  \n",
    "    3) Loop over each as_of_date slice, run PCA & log.  \n",
    "    4) After the loop, build a consolidated screeâ€plot or CSV if desired.\n",
    "    \"\"\"\n",
    "    end_date = date.today()\n",
    "    earliest_possible = as_of - relativedelta(years=ROLLING_YEARS) - relativedelta(days=days)\n",
    "    min_date = date(2010, 3, 15)\n",
    "    if earliest_possible < min_date:\n",
    "        earliest_possible = min_date\n",
    "\n",
    "    # 1) ONEâ€TIME: load & pivot entire range\n",
    "    print(f\"Loading data from {earliest_possible} to {end_date} (oneâ€time)...\")\n",
    "    pivot_filled = load_and_pivot_all(earliest_possible, end_date)\n",
    "\n",
    "    # 2) Start MLflow parent run\n",
    "    with mlflow.start_run(run_name=\"Rolling PCA\", nested=False):\n",
    "        mlflow.log_param(\"days_requested\", days)\n",
    "        mlflow.log_param(\"rolling_years\", ROLLING_YEARS)\n",
    "        mlflow.log_param(\"n_components\", N_COMPONENTS)\n",
    "        mlflow.log_param(\"curve_type\", CURVE_TYPE)\n",
    "        mlflow.log_param(\"pca_model\", pca_model.__name__)\n",
    "        mlflow.log_param(\"starting_domino_user\", os.environ.get(\"DOMINO_STARTING_USERNAME\", \"\"))\n",
    "\n",
    "        # We'll collect all explained_variance_ratios to make one scree plot at the end\n",
    "        scree_data = []\n",
    "\n",
    "        # 3) Loop over each day\n",
    "        for i in range(days):\n",
    "            as_of_date = as_of - relativedelta(days=i)\n",
    "            print(f\"â†’ Running PCA for {as_of_date}...\")\n",
    "\n",
    "            # Start a nested run for this date\n",
    "            with mlflow.start_run(nested=True, run_name=f\"PCA_{as_of_date}\") as nested_run:\n",
    "                global mlflow_start_time_per_slice\n",
    "                mlflow_start_time_per_slice = (time.time(),)  # just to measure perâ€slice latency\n",
    "                explained_ratio = run_pca_and_log_slice(as_of_date, pivot_filled)\n",
    "                scree_data.append((as_of_date, explained_ratio))\n",
    "                mlflow.end_run()\n",
    "\n",
    "        # 4) After all slices are done, optionally write a combined screeâ€plot & CSV once:\n",
    "        #    This avoids ğ file writes â‡’ only 1 final write.\n",
    "        all_components = pd.DataFrame(\n",
    "            {\n",
    "                \"as_of_date\": [d for d, ratios in scree_data],\n",
    "                **{\n",
    "                    f\"pc{i+1}_ratio\": [ratios[i] for d, ratios in scree_data]\n",
    "                    for i in range(N_COMPONENTS)\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "        # Save once:\n",
    "        csv_path = \"../../artifacts/results/all_scree_data.csv\"\n",
    "        all_components.to_csv(csv_path, index=False)\n",
    "        mlflow.log_artifact(csv_path, artifact_path=\"pca_metrics\")\n",
    "\n",
    "        # And make one combined screeâ€plot (chains of markers per date)\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        for as_of_date, ratios in scree_data:\n",
    "            ax.plot(\n",
    "                np.arange(1, N_COMPONENTS + 1),\n",
    "                ratios,\n",
    "                marker=\"o\",\n",
    "                linestyle=\"-\",\n",
    "                label=str(as_of_date),\n",
    "            )\n",
    "        ax.set_xlabel(\"Principal Component\")\n",
    "        ax.set_ylabel(\"Explained Variance Ratio\")\n",
    "        ax.set_title(f\"Scree Plot Over Time (last {days} days)\")\n",
    "        ax.legend(fontsize=\"small\", ncol=2, loc=\"upper right\", bbox_to_anchor=(1.2, 1.0))\n",
    "\n",
    "        plot_path = \"../../artifacts/results/all_scree_over_time.png\"\n",
    "        fig.savefig(plot_path, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(plot_path, artifact_path=\"scree_plots\")\n",
    "\n",
    "    print(\"âœ… All PCA runs complete.\")\n",
    "\n",
    "# â”€â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    default_backdated_days = 5\n",
    "    default_as_of = date.today()\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        try:\n",
    "            as_of = date.fromisoformat(sys.argv[1])\n",
    "        except ValueError:\n",
    "            as_of = default_as_of\n",
    "    else:\n",
    "        as_of = default_as_of\n",
    "\n",
    "    populate(default_backdated_days, as_of)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a932d-9e3c-41c9-8458-8936918a14c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
