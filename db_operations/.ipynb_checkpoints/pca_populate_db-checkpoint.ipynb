{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7a2c534-c854-4afa-8d8b-f2d0996485be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running for 2025-05-21\n",
      "âœ… PCA run complete (run_id: 786bedd3-e6ea-45ac-b1d2-a4b6ecc7eb1a). Logged metrics and artifact.\n",
      "ðŸƒ View run PCA_2025-05-21 at: http://127.0.0.1:8768/#/experiments/1443/runs/c2002423f2294f099a5c4c6e447697af\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1443\n",
      "running for 2025-05-20\n",
      "âœ… PCA run complete (run_id: 01707e92-92b4-4a59-9bec-116cd5efa0cf). Logged metrics and artifact.\n",
      "ðŸƒ View run PCA_2025-05-20 at: http://127.0.0.1:8768/#/experiments/1443/runs/5e8ca4ef550c477587cb257eccff8c35\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1443\n",
      "running for 2025-05-19\n",
      "âœ… PCA run complete (run_id: 3d48ce94-b16e-42f2-a446-0899f4119b31). Logged metrics and artifact.\n",
      "ðŸƒ View run PCA_2025-05-19 at: http://127.0.0.1:8768/#/experiments/1443/runs/1142b87c36ad438490ab15473255d6e0\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1443\n",
      "running for 2025-05-18\n",
      "âœ… PCA run complete (run_id: 01033281-210f-4f08-8332-e62414d72f19). Logged metrics and artifact.\n",
      "ðŸƒ View run PCA_2025-05-18 at: http://127.0.0.1:8768/#/experiments/1443/runs/e880f8bf980b444d850276ec975cf92c\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1443\n",
      "running for 2025-05-17\n",
      "âœ… PCA run complete (run_id: 8ef652c2-917d-4967-836d-f3003f6e62e8). Logged metrics and artifact.\n",
      "ðŸƒ View run PCA_2025-05-17 at: http://127.0.0.1:8768/#/experiments/1443/runs/3b03924396674e3fa56ed00b3e2a25ae\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1443\n",
      "ðŸƒ View run Rolling PCA at: http://127.0.0.1:8768/#/experiments/1443/runs/91a7d88c6a1b414ca7c68b3405a21669\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1443\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import mlflow\n",
    "from domino.data_sources import DataSourceClient\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "\n",
    "# â”€â”€â”€ CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TENORS = [0.25, 0.5, 1, 2, 3, 5, 7, 10, 20, 30]\n",
    "ROLLING_YEARS = 3\n",
    "N_COMPONENTS = 3\n",
    "CURVE_TYPE = \"US Treasury Par\"\n",
    "\n",
    "# MLflow experiment\n",
    "EXPERIMENT_NAME = \"PCA_Curve_Analysis\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# â”€â”€â”€ DATASOURCE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ds = DataSourceClient().get_datasource(\"market_data\")\n",
    "\n",
    "# â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@lru_cache(maxsize=100)\n",
    "def load_curve_data(start_date: date, end_date: date) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load treasury curves from the database between start_date and end_date.\n",
    "    \"\"\"\n",
    "    sql = f\"\"\"\n",
    "        SELECT curve_date, tenor_num AS tenor, rate\n",
    "          FROM rate_curves\n",
    "         WHERE curve_date BETWEEN '{start_date}' AND '{end_date}'\n",
    "           AND curve_type = '{CURVE_TYPE}'\n",
    "        ORDER BY curve_date\n",
    "    \"\"\"\n",
    "    df = ds.query(sql).to_pandas()\n",
    "    df['curve_date'] = pd.to_datetime(df['curve_date'])\n",
    "    return df\n",
    "\n",
    "# Updated `run_pca_and_log` with an existence check for the as_of_date\n",
    "\n",
    "    # Check if there's data for exactly as_of_date\n",
    "    available_dates = df['curve_date'].dt.date.unique()\n",
    "    if as_of_date not in available_dates:\n",
    "        print(f\"No curve data for {as_of_date}, skipping PCA.\")\n",
    "        return\n",
    "\n",
    "    # Build pivot and include only dates up to the target\n",
    "    pivot = (\n",
    "        df[df['curve_date'].dt.date <= as_of_date]\n",
    "          .pivot(index=\"curve_date\", columns=\"tenor\", values=\"rate\")\n",
    "          .reindex(columns=[t for t in TENORS if t in df['tenor'].unique()])\n",
    "          .ffill()\n",
    "          .bfill()\n",
    "    )\n",
    "\n",
    "    # Now pivot.index[-1].date() is guaranteed to be as_of_date\n",
    "    X = pivot.to_numpy()\n",
    "    num_obs, num_tenors = X.shape\n",
    "\n",
    "    # Proceed with PCA as before...\n",
    "    pca = PCA(n_components=N_COMPONENTS)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    # ... rest of the logic ...\n",
    "\n",
    "    print(f\"PCA run for {as_of_date} completed with {num_obs} observations.\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€ PCA + DB INSERT + MLflow LOGGING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_pca_and_log(as_of_date: date):\n",
    "    # Calculate rolling window dates\n",
    "    start_date = as_of_date - relativedelta(years=ROLLING_YEARS)\n",
    "    end_date = as_of_date\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load data\n",
    "    df = load_curve_data(start_date, end_date)\n",
    "    pivot = (\n",
    "        df.pivot(index=\"curve_date\", columns=\"tenor\", values=\"rate\")\n",
    "          .reindex(columns=[t for t in TENORS if t in df['tenor'].unique()])\n",
    "    )\n",
    "    pivot = pivot.ffill().bfill()\n",
    "    X = pivot.to_numpy()\n",
    "    num_obs, num_tenors = X.shape\n",
    "\n",
    "    # Fit PCA\n",
    "    pca = PCA(n_components=N_COMPONENTS)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    X_recon = pca.inverse_transform(X_pca)\n",
    "    recon_errors = (X - X_recon) ** 2\n",
    "    mse = recon_errors.mean()\n",
    "    mlflow.log_metric(\"reconstruction_mse\", float(mse))\n",
    "\n",
    "    # Today's scores\n",
    "    today_curve = pivot.iloc[-1].to_numpy()\n",
    "    today_scores = pca.transform([today_curve])[0]\n",
    "\n",
    "    # Compute run metrics\n",
    "    explained_ratio = pca.explained_variance_ratio_\n",
    "    total_explained = float(explained_ratio.sum())\n",
    "    run_duration = time.time() - start_time\n",
    "\n",
    "    stats = {\n",
    "        \"run_duration\": run_duration,\n",
    "        \"reconstruction_mse\": float(mse),\n",
    "        \"pc1_variance\":       float(explained_ratio[0]),\n",
    "        \"pc2_variance\":       float(explained_ratio[1]),\n",
    "        \"pc3_variance\":       float(explained_ratio[2]),\n",
    "        \"total_explained\":    total_explained\n",
    "    }\n",
    "    with open(\"dominostats.json\", \"w\") as f:\n",
    "        json.dump(stats, f)\n",
    "\n",
    "    # Generate a run_id\n",
    "    run_id = str(uuid.uuid4())\n",
    "\n",
    "    # Insert into DB\n",
    "    insert_sql = f\"\"\"\n",
    "    INSERT INTO curve_pca_results (\n",
    "      run_id, curve_type, curve_date, n_components,\n",
    "      total_explained_variance_ratio, explained_variance_ratios,\n",
    "      mean_curve, components, scores\n",
    "    ) VALUES (\n",
    "      '{run_id}', '{CURVE_TYPE}', '{as_of_date}',\n",
    "      {N_COMPONENTS}, {total_explained},\n",
    "      ARRAY{explained_ratio.tolist()},\n",
    "      ARRAY{pca.mean_.tolist()},\n",
    "      '{json.dumps(pca.components_.tolist()).replace(\"'\", \"''\")}',\n",
    "      ARRAY{today_scores.tolist()}\n",
    "    )\n",
    "    ON CONFLICT (curve_type, curve_date)\n",
    "    DO UPDATE SET\n",
    "      run_id                        = EXCLUDED.run_id,\n",
    "      n_components                  = EXCLUDED.n_components,\n",
    "      total_explained_variance_ratio= EXCLUDED.total_explained_variance_ratio,\n",
    "      explained_variance_ratios     = EXCLUDED.explained_variance_ratios,\n",
    "      mean_curve                    = EXCLUDED.mean_curve,\n",
    "      components                    = EXCLUDED.components,\n",
    "      scores                        = EXCLUDED.scores;\n",
    "    \"\"\"\n",
    "    ds.query(insert_sql)\n",
    "    \n",
    "    # MLflow logging\n",
    "    mlflow.log_param(\"as_of_date\", as_of_date)\n",
    "    mlflow.log_param(\"as_of_date_ordinal\", as_of_date.toordinal() - 733773)\n",
    "    mlflow.log_param(\"rolling_years\", ROLLING_YEARS)\n",
    "    mlflow.log_param(\"n_components\", N_COMPONENTS)\n",
    "    mlflow.log_param(\"curve_type\", CURVE_TYPE)\n",
    "    mlflow.log_param(\"num_tenors\", num_tenors)\n",
    "    mlflow.log_param(\"num_observations\", num_obs)\n",
    "\n",
    "    mlflow.log_metric(\"total_explained_variance\", total_explained)\n",
    "    for i, ratio in enumerate(explained_ratio, start=1):\n",
    "        mlflow.log_metric(f\"explained_variance_ratio_{i}\", float(ratio))\n",
    "    mlflow.log_metric(\"run_duration_seconds\", run_duration)\n",
    "\n",
    "    # Create DataFrame for artifact\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'component': list(range(1, N_COMPONENTS+1)),\n",
    "        'explained_variance_ratio': explained_ratio\n",
    "    })\n",
    "    metrics_df['cumulative_variance'] = metrics_df['explained_variance_ratio'].cumsum()\n",
    "\n",
    "    # Save artifact\n",
    "    csv_path = \"../../artifacts/results/rate_curves_loaded.csv\"\n",
    "    metrics_df.to_csv(csv_path, index=False)\n",
    "    mlflow.log_artifact(csv_path, artifact_path=\"pca_metrics\")\n",
    "    # mlflow.sklearn.log_model(pca, artifact_path=\"pca_model\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(\n",
    "        np.arange(1, N_COMPONENTS + 1),\n",
    "        pca.explained_variance_ratio_,\n",
    "        marker='o',\n",
    "        linestyle='-',\n",
    "    )\n",
    "    ax.set_xlabel(\"Principal Component\")\n",
    "    ax.set_ylabel(\"Explained Variance Ratio\")\n",
    "    ax.set_title(f\"Scree Plot (as_of={as_of_date})\")\n",
    "    \n",
    "    # 2) save locally\n",
    "    plot_path = f\"../../artifacts/results/scree_{as_of_date}.png\"\n",
    "    fig.savefig(plot_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 3) log to MLflow\n",
    "    mlflow.log_artifact(plot_path, artifact_path=\"scree_plots\")\n",
    "\n",
    "    print(f\"âœ… PCA run complete (run_id: {run_id}). Logged metrics and artifact.\")\n",
    "\n",
    "def populate(days, starting_date):\n",
    "    d = starting_date\n",
    "    start_time = time.time()\n",
    "    end_date = date.today()\n",
    "    start_date = end_date - relativedelta(days=days)\n",
    "    min_date = date(2010, 3, 15)\n",
    "    unique_dates = set()\n",
    "    if start_date < min_date:\n",
    "        start_date = min_date\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Rolling PCA\", nested=False):\n",
    "        mlflow.log_param(\"days_requested\", days)\n",
    "        mlflow.log_param(\"starting_domino_user\", os.environ[\"DOMINO_STARTING_USERNAME\"])\n",
    "\n",
    "        for i in range(days):\n",
    "            as_of_date = d - relativedelta(days=i)\n",
    "            print(f'running for {as_of_date}')\n",
    "            with mlflow.start_run(nested=True, run_name=f\"PCA_{as_of_date}\"):\n",
    "                run_pca_and_log(as_of_date)\n",
    "                mlflow.end_run()\n",
    "\n",
    "# â”€â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \n",
    "default_backdated_days = 5000\n",
    "default_as_of = date.today()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) > 1:\n",
    "        try:\n",
    "            as_of = date.fromisoformat(sys.argv[1])\n",
    "        except ValueError:\n",
    "            as_of = default_as_of\n",
    "    else:\n",
    "        as_of = default_as_of\n",
    "\n",
    "    populate(default_backdated_days, as_of)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a932d-9e3c-41c9-8458-8936918a14c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
