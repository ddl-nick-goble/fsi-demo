{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2c534-c854-4afa-8d8b-f2d0996485be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 1995\n",
      "  1995: fetched 8110 rows\n",
      "year 1996\n",
      "  1996: fetched 7945 rows\n",
      "year 1997\n",
      "  1997: fetched 7775 rows\n",
      "year 1998\n",
      "  1998: fetched 7611 rows\n",
      "year 1999\n",
      "  1999: fetched 7452 rows\n",
      "year 2000\n",
      "  2000: fetched 7297 rows\n",
      "year 2001\n",
      "  2001: fetched 7153 rows\n",
      "year 2002\n",
      "  2002: fetched 6990 rows\n",
      "year 2003\n",
      "  2003: fetched 6803 rows\n",
      "year 2004\n",
      "  2004: fetched 6602 rows\n",
      "year 2005\n",
      "  2005: fetched 6382 rows\n",
      "year 2006\n",
      "  2006: fetched 6163 rows\n",
      "year 2007\n",
      "  2007: fetched 5945 rows\n",
      "year 2008\n",
      "  2008: fetched 5725 rows\n",
      "year 2009\n",
      "  2009: fetched 5464 rows\n",
      "year 2010\n",
      "  2010: fetched 5177 rows\n",
      "year 2011\n",
      "  2011: fetched 4876 rows\n",
      "year 2012\n",
      "  2012: fetched 4610 rows\n",
      "year 2013\n",
      "  2013: fetched 4343 rows\n",
      "year 2014\n",
      "  2014: fetched 4079 rows\n",
      "year 2015\n",
      "  2015: fetched 3809 rows\n",
      "year 2016\n",
      "  2016: fetched 3534 rows\n",
      "year 2017\n",
      "  2017: fetched 3271 rows\n",
      "year 2018\n",
      "  2018: fetched 2994 rows\n",
      "year 2019\n",
      "  2019: fetched 2712 rows\n",
      "year 2020\n",
      "  2020: fetched 2385 rows\n",
      "year 2021\n",
      "  2021: fetched 1883 rows\n",
      "year 2022\n",
      "  2022: fetched 1437 rows\n",
      "year 2023\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from domino.data_sources import DataSourceClient\n",
    "\n",
    "API_BASE   = \"https://api.fiscaldata.treasury.gov/services/api/fiscal_service/v1/accounting/od/auctions_query\"\n",
    "PAGE_SIZE  = 10000   # plenty for a single year\n",
    "BATCH_SIZE = 5000     # rows per upsert batch\n",
    "\n",
    "mlflow.set_experiment(\"Populate DB (treasury_auction_results)\")\n",
    "ds = DataSourceClient().get_datasource(\"market_data\")\n",
    "\n",
    "def fetch_all_auctions_last_y(years_to_backfill) -> list[dict]:\n",
    "    rows = []\n",
    "    start_year = (date.today() - relativedelta(years=years_to_backfill)).year\n",
    "    end_year   = date.today().year\n",
    "\n",
    "    for yr in range(start_year, end_year + 1):\n",
    "        print('year', yr)\n",
    "        start = date(yr, 1, 1).isoformat()\n",
    "        end   = date(yr, 12, 31).isoformat()\n",
    "        params = [\n",
    "            (\"filter\", f\"record_date:gte:{start}\"),\n",
    "            (\"filter\", f\"record_date:lte:{end}\"),\n",
    "            (\"page[size]\", str(PAGE_SIZE)),\n",
    "            (\"sort\", \"record_date\"),\n",
    "        ]\n",
    "        resp = requests.get(API_BASE, params=params)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json().get(\"data\", [])\n",
    "        print(f\"  {yr}: fetched {len(data)} rows\")\n",
    "        rows.extend(data)\n",
    "    return rows\n",
    "\n",
    "def quote_value(val):\n",
    "    if val is None or val == \"null\":\n",
    "        return \"NULL\"\n",
    "    try:\n",
    "        float(val)\n",
    "        return str(val)\n",
    "    except:\n",
    "        return \"'\" + str(val).replace(\"'\", \"''\") + \"'\"\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "def dedupe(records: list[dict]) -> list[dict]:\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for r in records:\n",
    "        key = (r.get(\"record_date\"), r.get(\"cusip\"))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        deduped.append(r)\n",
    "    return deduped\n",
    "\n",
    "def upsert_batch(records: list[dict], batch_size: int = BATCH_SIZE):\n",
    "    if not records:\n",
    "        return\n",
    "    cols = list(records[0].keys())\n",
    "    col_list = \", \".join(cols)\n",
    "    total = len(records)\n",
    "    print(f\"Upserting {total} rows in batches of {batch_size}...\")\n",
    "    for idx, chunk in enumerate(chunks(records, batch_size), start=1):\n",
    "        print(f\"  Batch {idx}: {len(chunk)} rows\")\n",
    "        values_sql = []\n",
    "        for r in chunk:\n",
    "            vals = [quote_value(r.get(c)) for c in cols]\n",
    "            values_sql.append(\"(\" + \", \".join(vals) + \")\")\n",
    "        values_str = \",\\n\".join(values_sql)\n",
    "        set_list = \", \".join([f\"{c} = EXCLUDED.{c}\" for c in cols])\n",
    "        sql = f\"\"\"\n",
    "        INSERT INTO treasury_auction_results ({col_list})\n",
    "        VALUES\n",
    "        {values_str}\n",
    "        ON CONFLICT (record_date, cusip)\n",
    "        DO UPDATE SET\n",
    "          {set_list};\n",
    "        \"\"\"\n",
    "        ds.query(sql)\n",
    "\n",
    "def main(years_to_backfill):\n",
    "    with mlflow.start_run(run_name=\"Load 30yr Auction Results\"):\n",
    "        t0 = time.time()\n",
    "\n",
    "        data = fetch_all_auctions_last_y(years_to_backfill)\n",
    "        mlflow.log_metric(\"rows_fetched\", len(data))\n",
    "        data = dedupe(data)\n",
    "        upsert_batch(data)\n",
    "\n",
    "        duration = time.time() - t0\n",
    "        mlflow.log_metric(\"duration_seconds\", duration)\n",
    "        print(f\"âœ… Loaded {len(data)} rows in {duration:.1f}s\")\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        out = Path(\"artifacts\") / \"results\" / \"treasury_auction_results.csv\"\n",
    "        out.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(out, index=False)\n",
    "        mlflow.log_artifact(str(out), artifact_path=\"treasury_auction_results\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(years_to_backfill = 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d5914-6d2e-403b-8812-567f4d08b4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
