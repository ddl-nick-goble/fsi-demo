{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7a2c534-c854-4afa-8d8b-f2d0996485be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run trusting-gnu-927 at: http://127.0.0.1:8768/#/experiments/1441/runs/eae5da2fa68e4812b5932581e765fc08\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1441\n",
      "âœ… PCA pipeline done (run_id=eae5da2fa68e4812b5932581e765fc08, curve_type=US Treasury Par, as_of_date=2025-05-19)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "from datetime import date, datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.decomposition import PCA\n",
    "from domino.data_sources import DataSourceClient\n",
    "\n",
    "# â”€â”€â”€ PCA WITH PREDICT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class PCAWithPredict(PCA):\n",
    "    def predict(self, X):\n",
    "        return self.transform(X)\n",
    "\n",
    "# â”€â”€â”€ DEFAULTS & ARG PARSING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "default_window_months   = 3*12\n",
    "default_n_components    = 5\n",
    "default_svd_solver      = \"auto\"\n",
    "default_whiten          = False\n",
    "default_tol             = 0.0\n",
    "default_curve_type      = \"US Treasury Par\"\n",
    "default_as_of_date      = date.today()\n",
    "\n",
    "args = sys.argv[1:]\n",
    "try:\n",
    "    window_months = int(args[0])\n",
    "except:\n",
    "    window_months = default_window_months\n",
    "\n",
    "try:\n",
    "    n_components = int(args[1])\n",
    "except:\n",
    "    n_components = default_n_components\n",
    "\n",
    "try:\n",
    "    svd_solver = args[2] if args[2] in (\"auto\",\"full\",\"arpack\",\"randomized\") else default_svd_solver\n",
    "except:\n",
    "    svd_solver = default_svd_solver\n",
    "\n",
    "try:\n",
    "    whiten = str(args[3]).lower() in (\"true\",\"1\",\"yes\")\n",
    "except:\n",
    "    whiten = default_whiten\n",
    "\n",
    "try:\n",
    "    tol = float(args[4])\n",
    "except:\n",
    "    tol = default_tol\n",
    "\n",
    "try:\n",
    "    curve_type = args[5]\n",
    "except:\n",
    "    curve_type = default_curve_type\n",
    "\n",
    "try:\n",
    "    as_of_date = datetime.strptime(args[6], '%Y-%m-%d').date()\n",
    "except:\n",
    "    as_of_date = default_as_of_date\n",
    "\n",
    "# â”€â”€â”€ MAIN PIPELINE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main(\n",
    "    window_months: int,\n",
    "    n_components: int,\n",
    "    svd_solver: str,\n",
    "    whiten: bool,\n",
    "    tol: float,\n",
    "    curve_type: str,\n",
    "    as_of_date: date\n",
    "):\n",
    "    ds = DataSourceClient().get_datasource(\"market_data\")\n",
    "    end_date = as_of_date\n",
    "    start_date = end_date - relativedelta(months=window_months)\n",
    "\n",
    "    # 1) Fetch curve data for the specified type\n",
    "    sql = f\"\"\"\n",
    "        SELECT curve_date, tenor_num, rate\n",
    "          FROM rate_curves\n",
    "         WHERE curve_type = '{curve_type}'\n",
    "           AND curve_date BETWEEN '{start_date}' AND '{end_date}'\n",
    "         ORDER BY curve_date, tenor_num;\n",
    "    \"\"\"\n",
    "    df = ds.query(sql).to_pandas()\n",
    "\n",
    "    pivot = (\n",
    "        df\n",
    "        .pivot(index=\"curve_date\", columns=\"tenor_num\", values=\"rate\")\n",
    "        .sort_index()\n",
    "        .interpolate(axis=1).ffill(axis=1).bfill(axis=1)\n",
    "        .dropna(axis=0)\n",
    "    )\n",
    "    dates = pivot.index.to_list()\n",
    "    X = pivot.values\n",
    "    \n",
    "    mlflow.set_experiment(\"Curve PCA history test\")\n",
    "    with mlflow.start_run() as run:\n",
    "        run_id = run.info.run_id\n",
    "    \n",
    "        # Log hyperparameters\n",
    "        mlflow.log_params({\n",
    "            \"window_months\": window_months,\n",
    "            \"n_components\": n_components,\n",
    "            \"svd_solver\": svd_solver,\n",
    "            \"whiten\": whiten,\n",
    "            \"tol\": tol,\n",
    "            \"curve_type\": curve_type,\n",
    "            \"as_of_date\": str(as_of_date)\n",
    "        })\n",
    "    \n",
    "        # 2) Fit our subclassed PCA\n",
    "        pca = PCAWithPredict(\n",
    "            n_components=n_components,\n",
    "            svd_solver=svd_solver,\n",
    "            whiten=whiten,\n",
    "            tol=tol\n",
    "        )\n",
    "        scores = pca.fit_transform(X)\n",
    "    \n",
    "        # 3) Log explained-variance metrics\n",
    "        total_var = 0.0\n",
    "        for i, var in enumerate(pca.explained_variance_ratio_, start=1):\n",
    "            mlflow.log_metric(f\"var_deg_{i}\", float(var))\n",
    "            total_var += float(var)\n",
    "        mlflow.log_metric(\"var_total\", total_var)\n",
    "\n",
    "        # 4) Build input example & signature\n",
    "        input_example = pd.DataFrame(X[:3, :], columns=pivot.columns).head(1)\n",
    "        signature = infer_signature(input_example, pca.transform(input_example))\n",
    "    \n",
    "        # 5) Log the model (with signature & python_function flavor)\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=pca,\n",
    "            artifact_path=\"pca_model\",\n",
    "            input_example=input_example,\n",
    "            signature=signature\n",
    "        )\n",
    "\n",
    "        # 6) Build PCA results records\n",
    "        evr = pca.explained_variance_ratio_.tolist()\n",
    "        comps_json = json.dumps(pca.components_.tolist())\n",
    "        records = []\n",
    "        for idx, dt in enumerate(dates):\n",
    "            for comp_idx in range(n_components):\n",
    "                records.append({\n",
    "                    \"run_id\": run_id,\n",
    "                    \"curve_type\": curve_type,\n",
    "                    \"n_components\": n_components,\n",
    "                    \"explained_variance_ratio\": evr,\n",
    "                    \"components\": comps_json,\n",
    "                    \"curve_date\": dt.isoformat(),\n",
    "                    \"component_index\": comp_idx + 1,\n",
    "                    \"score\": float(scores[idx, comp_idx]),\n",
    "                })\n",
    "\n",
    "        # 7) Batch-insert into curve_pca_results\n",
    "        BATCH = 500\n",
    "        for i in range(0, len(records), BATCH):\n",
    "            chunk = records[i : i + BATCH]\n",
    "            vals = \",\".join(\n",
    "                f\"('{r['run_id']}','{r['curve_type']}',CURRENT_TIMESTAMP,\"\n",
    "                f\"{r['n_components']},ARRAY{r['explained_variance_ratio']},\"\n",
    "                f\"'{r['components']}'::jsonb,'{r['curve_date']}',\"\n",
    "                f\"{r['component_index']},{r['score']})\"\n",
    "                for r in chunk\n",
    "            )\n",
    "            insert_sql = f\"\"\"\n",
    "            INSERT INTO curve_pca_results\n",
    "              (run_id, curve_type, run_timestamp, n_components,\n",
    "               explained_variance_ratio, components, curve_date,\n",
    "               component_index, score)\n",
    "            VALUES {vals}\n",
    "            ON CONFLICT (run_id, curve_type, curve_date, component_index)\n",
    "              DO NOTHING;\n",
    "            \"\"\"\n",
    "            ds.query(insert_sql)\n",
    "\n",
    "    print(f\"âœ… PCA pipeline done (run_id={run_id}, curve_type={curve_type}, as_of_date={as_of_date})\")\n",
    "\n",
    "# â”€â”€â”€ ENTRY POINT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    main(window_months, n_components, svd_solver, whiten, tol, curve_type, as_of_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d5914-6d2e-403b-8812-567f4d08b4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
